{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/INFO4080/blob/main/Week_02-The_Research_Question/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ykJsxgvIBD"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Natural_language_processing\n",
        "* https://www.nltk.org/index.html\n",
        "* https://spacy.io/\n",
        "* https://pypi.org/project/wikipedia/ # pie pea eye or python packaging index\n",
        "* https://kgextension.readthedocs.io/en/latest/\n",
        "\n",
        "NLTK Downloads\n",
        "\n",
        "* install nltk: https://pypi.org/project/nltk/\n",
        "* stopwords: https://pythonspot.com/nltk-stop-words/\n",
        "* punkt: https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
        "* wordnet: https://www.tutorialspoint.com/how-to-get-synonyms-antonyms-from-nltk-wordnet-in-python\n",
        "* averaged_perceptron_tagger: https://morioh.com/p/04a148fa2131"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JczCBHL3vIBG"
      },
      "outputs": [],
      "source": [
        "# downloads for processing raw text, meanings, pos tagging, and cleaning\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw4gSDl4vIBH"
      },
      "source": [
        "## Text Analysis\n",
        "\n",
        "From https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction :\n",
        "\n",
        "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
        "\n",
        "> In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
        "> * tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators\n",
        "> * counting the occurrences of tokens in each document\n",
        "> * normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents\n",
        "\n",
        "> In this scheme, features and samples are defined as follows:\n",
        "> * each individual token occurrence frequency (normalized or not) is treated as a feature\n",
        "> * the vector of all the token frequencies for a given document is considered a multivariate sample\n",
        "> A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus\n",
        "\n",
        "> We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2brRhINvIBI"
      },
      "source": [
        "## Word Tokens\n",
        "\n",
        "Tokens are the total numbers of words in a corpus regardless if they are repeated. Word tokenization splits text into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTxm4ylIvIBI"
      },
      "outputs": [],
      "source": [
        "# demonstrate word_tokenize\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# text = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
        "# word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hukhByKmvIBJ"
      },
      "source": [
        "## CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rAlbgJovIBJ"
      },
      "outputs": [],
      "source": [
        "# create dataframe from messages\n",
        "# import pandas as pd\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# msgs = [\n",
        "#     'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns',\n",
        "#     'Learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
        "# ]\n",
        "\n",
        "# df = pd.DataFrame({'msgs': msgs})\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOs7uDqAvIBK"
      },
      "outputs": [],
      "source": [
        "# demonstrate CountVectorizer\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# cv = CountVectorizer()\n",
        "# matrix = cv.fit_transform(df['msgs'])\n",
        "# cv_df = pd.DataFrame(matrix.toarray(), columns=cv.get_feature_names_out())\n",
        "# cv_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUyqCW0VvIBK"
      },
      "source": [
        "## Bag of Words\n",
        "\n",
        "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Bag-of-words_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErOXRLN8vIBK"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming finds the stem of a word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qgWKGpbvIBL"
      },
      "outputs": [],
      "source": [
        "# demonstrate stemming\n",
        "# from nltk.stem import PorterStemmer\n",
        "\n",
        "# ps =PorterStemmer()\n",
        "# words= ['learn', 'learned', 'learning', 'learns']\n",
        "\n",
        "# for word in words:\n",
        "#     print(ps.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UYhj3jzvIBL"
      },
      "outputs": [],
      "source": [
        "# demonstrate stemming and tokenization\n",
        "# from nltk.stem import PorterStemmer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# ps =PorterStemmer()\n",
        "# sentence = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
        "# words = word_tokenize(sentence)\n",
        "# print([ps.stem(word) for word in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K_FVczdvIBL"
      },
      "outputs": [],
      "source": [
        "# demonstrate stemming and tokenization\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# sb =SnowballStemmer(language='english')\n",
        "# sentence = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
        "# words = word_tokenize(sentence)\n",
        "# print([sb.stem(word) for word in words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOdNuB3AvIBM"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization tries to provide context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RnTfxVUvIBM"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsOgsIPGvIBM"
      },
      "outputs": [],
      "source": [
        "# demonstrate lemmatization\n",
        "# from nltk.corpus import wordnet as wn\n",
        "# from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# from nltk import word_tokenize, pos_tag\n",
        "# from collections import defaultdict\n",
        "\n",
        "# tag_map = defaultdict(lambda : wn.NOUN)\n",
        "# tag_map['J'] = wn.ADJ\n",
        "# tag_map['V'] = wn.VERB\n",
        "# tag_map['R'] = wn.ADV\n",
        "\n",
        "# text = 'learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
        "# tokens = word_tokenize(text)\n",
        "# lemma_function = WordNetLemmatizer()\n",
        "\n",
        "# for token, tag in pos_tag(tokens):\n",
        "#     lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
        "#     print(f'{token} => {lemma} ({tag_map[tag[0]]})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm-0waBevIBM"
      },
      "source": [
        "## TF-IDF (Term Frequency Inverse Document Frequency)\n",
        "\n",
        "* Term frequency vs term usefulness\n",
        "* Simple frequency count can be misleading because frequent terms in one document can also be frequent in other documents\n",
        "* TF-IDF is used to score words in context of the document as well as in the context of the corpus, the higher the score the more useful\n",
        "\n",
        "For example, you are wondering what to take for your electives. You want the class to be good but you also want the class to be relevant to your major. It's easy to see that the class can be:\n",
        "1. both good and relevant\n",
        "2. good but not relevant\n",
        "3. relevant but not good\n",
        "4. not good and not relevant\n",
        "\n",
        "In the same way, a term may be:\n",
        "1. frequently used in your corpus and useful in the analysis of the document it is found in\n",
        "2. frequently used in your corpus but useless\n",
        "3. infrequently used in your corpus but useful\n",
        "4. infrequenlty used in your corpus and useless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWm_cBXvIBN"
      },
      "outputs": [],
      "source": [
        "# tf-idf demonstration\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# text1 = 'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns.'\n",
        "# text2 = 'learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# vectors = vectorizer.fit_transform([text1, text2])\n",
        "# feature_names = vectorizer.get_feature_names_out()\n",
        "# dense = vectors.todense()\n",
        "# denselist = dense.tolist()\n",
        "# df = pd.DataFrame(denselist, columns=feature_names)\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOhEBb2vIBN"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8uBpZLevIBN"
      },
      "outputs": [],
      "source": [
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# # add words\n",
        "# add_stopwords = ['word1', 'word2']\n",
        "# stopwords = stopwords.union(add_stopwords)\n",
        "\n",
        "# # remove words\n",
        "# remove_stopwords = {'word1', 'word2'}\n",
        "# stopwords = set([word for word in stopwords if word not in remove_stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MEWOhKUvIBN"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/54366913/removing-stopwords-from-a-pandas-dataframe\n",
        "# import pandas as pd\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# msgs = [\n",
        "#     'I love learning. I have learned so much, and hope to learn more. I also hope to learn how a machine learns',\n",
        "#     'Learning about beautiful mice was so much fun till I learned there was going to be a quiz'\n",
        "# ]\n",
        "\n",
        "# df = pd.DataFrame({'msgs': msgs})\n",
        "# stopwords = set(stopwords.words('english'))\n",
        "# # df['msgs'] = df['msgs'].str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
        "# df['msgs'] = df['msgs'].apply(lambda x: ' '.join([item.lower() for item in x.split() if item.lower() not in stopwords]))\n",
        "# print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPeDhiWrvIBO"
      },
      "source": [
        "## spaCy\n",
        "\n",
        "Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it’s possible to solve some problems starting from only the raw characters, it’s usually better to use linguistic knowledge to add useful information. That’s exactly what spaCy is designed to do: you put in raw text, and get back a Doc object, that comes with a variety of annotations.\n",
        "\n",
        "https://spacy.io/usage/linguistic-features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy.cli\n",
        "\n",
        "# spacy.cli.download('en_core_web_sm')\n",
        "# nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "pvAr5kUTub4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSUw0bJnvIBO"
      },
      "outputs": [],
      "source": [
        "# pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# en_core_web_md is an English pipeline trained on written web text (blogs, news, comments),\n",
        "# that includes vocabulary, syntax, entities, and vectors\n",
        "# import spacy\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAJQOALNvIBO"
      },
      "source": [
        "## Language Models\n",
        "\n",
        "1. Artificial Intelligence - what we would consider intelligent activity\n",
        "2. Machine Learning - Performance improvement based on a Task and Experience\n",
        "3. Neural Nets - A network of activation functions\n",
        "4. Deep Learning - Layers of various activation functions\n",
        "5. Language Model - Fill in the blank, complete the sentence given n-grams\n",
        "6. Large Language Model - Millions and billions of trainable weights\n",
        "\n",
        "### N-grams\n",
        "\n",
        "* Sequence of n successive words\n",
        "* Unigrams, bigrams, trigrams, n-grams\n",
        "* n = 3; I love learning, love learning I, learning I have, I have learned...\n",
        "\n",
        "### Markov Chains\n",
        "\n",
        "* Markov chains are used to generate a sequence of words that form a complete sentence\n",
        "* A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event\n",
        "* Informally, this may be thought of as, \"What happens next depends only on the state of affairs now\"\n",
        "\n",
        "https://en.wikipedia.org/wiki/Markov_chain\n",
        "\n",
        "A language model is a **probability distribution over sequences of words**. Given any sequence of words of length m, a language model **assigns a probability ... to the whole sequence**. Language models generate probabilities by **training on text corpora in one or many languages**. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data... Language models are useful for a variety of problems in computational linguistics; from initial **applications in speech recognition** to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in **machine translation** (e.g. scoring candidate translations), **natural language generation (generating more human-like text)**, **part-of-speech tagging, parsing, optical character recognition, handwriting recognition, grammar induction, information retrieval, etc**. Since 2018, **large language models (LLMs) consisting of deep neural networks with billions of trainable parameters, trained on massive datasets of unlabelled text**, have demonstrated impressive results on a wide variety of natural language processing tasks. This development has led to a shift in research focus toward the use of general-purpose LLMs.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Language_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "* https://www.youtube.com/watch?v=viZrOnJclY0\n",
        "* https://aneesha.medium.com/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
        "* In natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n",
        "Source\n",
        "* https://en.wikipedia.org/wiki/Word_embedding"
      ],
      "metadata": {
        "id": "36cd7z3vvn3a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDwSDIYYvIBO"
      },
      "source": [
        "## Similarity Measures\n",
        "\n",
        "https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55\n",
        "\n",
        "\n",
        "### Longest Common Substring\n",
        "\n",
        "* India and Indiana would return 5\n",
        "\n",
        "### Levenshtein Edit Distance\n",
        "\n",
        "* Finds the minimum number of single-character edits such as replacement, deletion, and insertion, needed to convert 1 text into another\n",
        "* India and Indiana would return 2\n",
        "\n",
        "### Hamming Distance\n",
        "\n",
        "* Finds the number replacements needed to change one text into another of equal size\n",
        "* Indians and Indiana returns 2\n",
        "\n",
        "### Jaccard Distance\n",
        "\n",
        "* Finds how disimilar two words are by distance and the lower the distance, the more similar\n",
        "\n",
        "### Euclidean Distance\n",
        "\n",
        "* Finds the length between two points\n",
        "* l2 norm\n",
        "\n",
        "### Dot Product\n",
        "\n",
        "* Considers orientation, the direction, that Euclidean lacks\n",
        "* Uses magnitude with orientation\n",
        "\n",
        "### Cosine Similarity\n",
        "\n",
        "In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval -1, 1. For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1. For example, in information retrieval and text mining, each word is assigned a different coordinate and a document is represented by the vector of the numbers of occurrences of each word in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be, in terms of their subject matter, and independently of the length of the documents. The technique is also used to measure cohesion within clusters in the field of data mining.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm40n8IyvIBP"
      },
      "outputs": [],
      "source": [
        "# demonstrate CountVectorizer\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# text1 = 'Cricket is very popular in India especially in May with the IPL'\n",
        "# text2 = 'Every May, the Indianapolis 500 runs in Indiana'\n",
        "# text3 = 'The Indian Premier Cricket League starts in March and continues through May'\n",
        "\n",
        "# msgs = [text1, text2]\n",
        "\n",
        "# cv = CountVectorizer()\n",
        "# matrix = cv.fit_transform(msgs)\n",
        "# cv_df = pd.DataFrame(matrix.toarray(), columns=cv.get_feature_names_out())\n",
        "# cv_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZgBwQmZvIBP"
      },
      "source": [
        "* Rows are word vectors\n",
        "* Columns are document vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ay1WdzvIBP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# vectors = vectorizer.fit_transform([text1, text2, text3])\n",
        "# feature_names = vectorizer.get_feature_names_out()\n",
        "# dense = vectors.todense()\n",
        "# denselist = dense.tolist()\n",
        "# df = pd.DataFrame(denselist, columns=feature_names)\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t9e3BK2vIBP"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/53453559/similarity-in-spacy\n",
        "# import numpy as np\n",
        "\n",
        "# v1 = nlp(text1)\n",
        "# v2 = nlp(text2)\n",
        "# v3 = nlp(text3)\n",
        "\n",
        "# print(f'spaCy: v1,v2={v1.similarity(v2)}; v1,v3={v1.similarity(v3)}; v2,v3={v2.similarity(v3)}')\n",
        "# print(np.dot(v1.vector, v2.vector) / (np.linalg.norm(v1.vector) * np.linalg.norm(v2.vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMPz7OcovIBQ"
      },
      "source": [
        "## Word Similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9h8w7aJvIBQ"
      },
      "outputs": [],
      "source": [
        "# tokens = nlp('dog cat banana afskfsd')\n",
        "\n",
        "# for token in tokens:\n",
        "#     print(token.text, token.has_vector, token.vector_norm, token.is_oov) # norm distance from origin, oov out-of-vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "norm in a word vector\n",
        "\n",
        "https://groups.google.com/g/word2vec-toolkit/c/PLKAKE2UqtI?pli=1"
      ],
      "metadata": {
        "id": "rkznNaoqs8zf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33k5xw8bvIBQ"
      },
      "source": [
        "## Part of Speech (POS)\n",
        "\n",
        "* Words are tagged within a grammatical framework\n",
        "* https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXpIfFjQvIBQ"
      },
      "outputs": [],
      "source": [
        "# create spacy doc with u(nicode) string\n",
        "# doc = nlp(u\"I like to read about data analysis everyday. I read a book on knowledge discovery last night.\")\n",
        "# print(doc.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNorO_uBvIBQ"
      },
      "outputs": [],
      "source": [
        "# for token in doc:\n",
        "#     print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVXli-jgvIBR"
      },
      "source": [
        "## Named Entity Recognition (NER)\n",
        "\n",
        "* Seeks and classifies text that might be considered proper nouns\n",
        "* GPE: Geographical Entity\n",
        "* Org: Organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTjDxfjQvIBR"
      },
      "outputs": [],
      "source": [
        "# doc = nlp(u\"I read about data analysis in Denton Texas everyday. I read a book on knowledge discovery last night from WikiData.\")\n",
        "# for ent in doc.ents:\n",
        "#     print(ent.text, ent.label_, str(spacy.explain(ent.label_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsl06exFvIBR"
      },
      "source": [
        "## Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYmmLQozvIBR"
      },
      "outputs": [],
      "source": [
        "# doc = nlp(u\"I read about data analysis in Denton Texas everyday. I read a book on knowledge discovery last night from WikiData.\")\n",
        "# for sent in doc.sents:\n",
        "#     print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41WKbQjRvIBR"
      },
      "source": [
        "## Topic Modeling\n",
        "\n",
        "Topic modeling helps us\n",
        "\n",
        "* discover hidden, or latent, topics, or themes in documents\n",
        "* summarize documents\n",
        "* search for similar documents\n",
        "* classify documents\n",
        "\n",
        "A document consists of topics and topics consist of words. The same word can be a part of multiple topics and one topic can be part of multiple documents. We can assign probabilities to how relevant a word is in one topic and that probability can be larger or smaller in another topic. The same can be said in the relationship between topics and documents. We can use topics and the words in topics for knowledge discovery without going through the entire document. These latent topics are like clustering, which we’ll cover a little more next week, and since they’re latent, we really don’t know, at first, what the big theme of the topic is so we just say that this group of words belong to topic 1, this group of words to topic 2, and so on. At some point, we might be able to give a topic a name, but it’s not necessary for our purposes. We want to find documents with similar topics because those topics have the same words, or key words, we’re looking for. In a sense, we can start annotating our documents by topics to optimize our searching.\n",
        "\n",
        "\n",
        "### Latent Dirichlet Allocation\n",
        "\n",
        "In natural language processing, Latent Dirichlet Allocation (LDA) is a generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar. LDA is an example of a topic model. In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
        "\n",
        "* Documents with similar topics use similar groups of words\n",
        "* Topics can be discovered (latent topics) by finding words that frequently occur together in a document\n",
        "* It's up to the user to label the topics based on the words within that topic (topic with the words Titanic and Carpathia might be Ship Tragedies)\n",
        "* LDA represents documents as probabilities of topics which consists of probabilities of words\n",
        "* LDA requires us to select the number of topics (K), the topics are are latent\n",
        "* Then we randomly assign words in a document to a K topic\n",
        "* Then we find the proportion of words assigned to the topic p(topic t | document d)\n",
        "* We also find p(word w | topic t)\n",
        "* Then we reassign the word to a new topic with p(topic t | document d) * p(word w | topic t)\n",
        "* This is the probability that the topic generated the word\n",
        "* This is done a large number of times till words to topics are acceptable (clustering)\n",
        "\n",
        "We have seen that some probabilities are associated with distributions. Rolling a die is associated with a Uniform Distribution, the number of successes in a sequence of trials is associated with a Binomial Distribution, etc. In the same sense, the probabilities p(topic t | document d) and p(word w | topic t), is associated with a Dirichlet Distribution.\n",
        "\n",
        "https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n",
        "\n",
        "https://highdemandskills.com/topic-modeling-intuitive/\n",
        "\n",
        "### Non-Negative Matrix Factorization\n",
        "\n",
        "Non-Negative Matrix Factorization is a statistical method that helps us to reduce the dimension of the input corpora or corpora. Internally, it uses the factor analysis method to give comparatively less weightage to the words that are having less coherence.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/\n",
        "\n",
        "* Performs dimensionality reduction and clustering\n",
        "* Used with TF-IDF"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}